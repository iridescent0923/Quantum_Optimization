{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Standard Library Imports\n",
    "# ==============================\n",
    "from enum import Enum\n",
    "import random\n",
    "\n",
    "# ==============================\n",
    "# Third-party Library Imports\n",
    "# ==============================\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Latex\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "import numpy as np  # Original numpy\n",
    "import pennylane as qml\n",
    "import scipy as sp\n",
    "\n",
    "import torch \n",
    "\n",
    "# Pennylane numpy\n",
    "from pennylane import numpy as pnp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================\n",
    "# Setup for Quantum Computations\n",
    "# ==============================\n",
    "\n",
    "# PennyLane settings\n",
    "dev = qml.device('default.mixed', wires=1)\n",
    "\n",
    "# Define Hamiltonian for quantum computations\n",
    "H = qml.Hamiltonian(coeffs = [-0.5], observables=[qml.PauliZ(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global Parameters\n",
    "Tau_global = 5e-2   # Dephase tau\n",
    "Paras_global = torch.zeros(2)\n",
    "Phi_global = torch.zeros(1)\n",
    "Gamma_ps = 0\n",
    "\n",
    "def Dephase_factor(tau):\n",
    "    \"\"\" Take tau and return gamma based on the following relation.\"\"\"\n",
    "    \n",
    "    return 1 - np.exp(-2 * tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch', diff_method='backprop')\n",
    "def circuit(phi):\n",
    "    global Paras_global\n",
    "    theta_x = Paras_global[0]\n",
    "    phi_z = Paras_global[1]\n",
    "    \n",
    "    gamma_dephase_in = Dephase_factor(Tau_global)  \n",
    "\n",
    "    qml.RX(pnp.pi/2, wires = 0)\n",
    "\n",
    "    qml.ApproxTimeEvolution(H, phi, 1)\n",
    "    qml.PhaseDamping(gamma_dephase_in, wires = 0) \n",
    "\n",
    "    qml.RZ(phi_z, wires = 0)  # phi_z\n",
    "    \n",
    "    qml.RX(theta_x, wires = 0)  # theta_x\n",
    "    \n",
    "    return qml.density_matrix(wires = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "@qml.qnode(dev, interface='torch', diff_method='backprop')\n",
    "def Post_selection_Dephase(phi):\n",
    "    \"\"\" Take qnode from circuit_1 and calculate decoherence using kraus operator.\n",
    "    \n",
    "    Args:\n",
    "        phi (float): Phi for Time-approximation. Pass by global variables:'Phi_global'\n",
    "\n",
    "    Returns:\n",
    "        qml.density_matrix: Density matrix of full qnode\n",
    "    \"\"\"\n",
    "    \n",
    "    global Paras_global, Gamma_ps\n",
    "    \n",
    "    density_matrix = circuit(phi)\n",
    "    qml.QubitDensityMatrix(density_matrix, wires = 0)\n",
    "    \n",
    "    # Kraus operator for 2*2 matrix\n",
    "    K = torch.tensor([\n",
    "        [pnp.sqrt(1 - Gamma_ps), 0],\n",
    "        [0, 1]\n",
    "    ], dtype=torch.complex128)\n",
    "    \n",
    "    Numerator = K @ density_matrix @ K.conj().T\n",
    "    Denominator = torch.trace(Numerator)\n",
    "    rho_ps = Numerator / Denominator\n",
    "\n",
    "    qml.QubitDensityMatrix(rho_ps, wires = 0)\n",
    "    \n",
    "    return qml.density_matrix(wires = 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paras_global[:2] = np.pi/4\n",
    "# Gamma_ps = 0\n",
    "# Tau_global = 0\n",
    "# Phi_global = torch.tensor([np.pi*2])\n",
    "\n",
    "# qml.qinfo.classical_fisher(Post_selection_Dephase)(Phi_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi_global.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cost_function(paras):\n",
    "    \"\"\" Calculate Classical-Fisher-Information for qnode(=Post_selection_Dephase).\n",
    "    \n",
    "    Args:\n",
    "        paras (Numpy array): [theta_init, tau_1, tau_2, tau_d1, tau_d2, tau_d3]\n",
    "\n",
    "    Returns:\n",
    "        _type_: CFI with minus(-) sign.\n",
    "    \"\"\"\n",
    "    \n",
    "    global Paras_global, Phi_global\n",
    "    Paras_global = paras\n",
    "          \n",
    "    CFI = qml.qinfo.classical_fisher(Post_selection_Dephase)(Phi_global)\n",
    "    \n",
    "    return -CFI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3333]], dtype=torch.float64, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras_test = torch.tensor([np.pi/4, np.pi/4])\n",
    "Phi_global = torch.tensor([np.pi*2])\n",
    "Tau_global = 0\n",
    "Gamma_ps = 0\n",
    "\n",
    "Cost_function(paras_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paras_test.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.3333]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.2861]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.2339]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.1788]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.1238]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0734]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0329]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0072]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0002]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0129]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0434]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.0874]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.1396]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.1950]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.2495]], dtype=torch.float64, grad_fn=<NegBackward0>)\n",
      "tensor([[-0.3004]], dtype=torch.float64, grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "source": [
    "PHI = torch.arange(0, np.pi/2, 1e-1)\n",
    "\n",
    "for phi_idx, phi_current in enumerate(PHI):\n",
    "    Phi_global = phi_current\n",
    "    print(Cost_function(paras_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "001th iteration, cost= -0.807178329712801\n",
      "002th iteration, cost= -0.8071797529325022\n",
      "003th iteration, cost= -0.8071804158314835\n",
      "004th iteration, cost= -0.8071810787305032\n",
      "005th iteration, cost= -0.8071817416295605\n",
      "006th iteration, cost= -0.8071824045286564\n",
      "007th iteration, cost= -0.8071830674277906\n",
      "008th iteration, cost= -0.8071837303269631\n",
      "009th iteration, cost= -0.8071864380900625\n",
      "010th iteration, cost= -0.807187100989388\n",
      "011th iteration, cost= -0.8071887752175828\n",
      "012th iteration, cost= -0.8071914829806265\n",
      "013th iteration, cost= -0.807191894872441\n",
      "014th iteration, cost= -0.8071932206715842\n",
      "015th iteration, cost= -0.8071938835712127\n",
      "016th iteration, cost= -0.8071945464708795\n",
      "017th iteration, cost= -0.8071950521346556\n",
      "018th iteration, cost= -0.8071962206978722\n",
      "019th iteration, cost= -0.8071989284618352\n",
      "020th iteration, cost= -0.807200254261347\n",
      "021th iteration, cost= -0.8072029620258335\n",
      "022th iteration, cost= -0.8072056136257731\n",
      "023th iteration, cost= -0.8072063739458717\n",
      "024th iteration, cost= -0.8072070368459502\n",
      "025th iteration, cost= -0.8072076997460665\n",
      "026th iteration, cost= -0.8072083626462208\n",
      "027th iteration, cost= -0.8072090255464135\n",
      "028th iteration, cost= -0.8072096884466438\n",
      "029th iteration, cost= -0.807212396212391\n",
      "030th iteration, cost= -0.8072130591127741\n",
      "031th iteration, cost= -0.8072157668789289\n",
      "032th iteration, cost= -0.8072178529999308\n",
      "033th iteration, cost= -0.807219178801188\n",
      "034th iteration, cost= -0.8072198417018737\n",
      "035th iteration, cost= -0.8072205046025975\n",
      "036th iteration, cost= -0.807221167503359\n",
      "037th iteration, cost= -0.8072218304041596\n",
      "038th iteration, cost= -0.8072224933049974\n",
      "039th iteration, cost= -0.8072252010724108\n",
      "040th iteration, cost= -0.8072265268744299\n",
      "041th iteration, cost= -0.8072271897754961\n",
      "042th iteration, cost= -0.807229897543545\n",
      "043th iteration, cost= -0.8072313207644426\n",
      "044th iteration, cost= -0.80723264656699\n",
      "045th iteration, cost= -0.8072333094683204\n",
      "046th iteration, cost= -0.8072339723696891\n",
      "047th iteration, cost= -0.8072346352710958\n",
      "048th iteration, cost= -0.8072352981725404\n",
      "049th iteration, cost= -0.8072380059416171\n",
      "050th iteration, cost= -0.8072393317448487\n",
      "051th iteration, cost= -0.8072406575482325\n",
      "052th iteration, cost= -0.8072433653180573\n",
      "053th iteration, cost= -0.8072454514412832\n",
      "054th iteration, cost= -0.80724611434322\n",
      "055th iteration, cost= -0.8072467772451944\n",
      "056th iteration, cost= -0.807247440147207\n",
      "057th iteration, cost= -0.8072481030492576\n",
      "058th iteration, cost= -0.8072486087074046\n",
      "059th iteration, cost= -0.8072513164778504\n",
      "060th iteration, cost= -0.8072524850376444\n",
      "061th iteration, cost= -0.8072538108419212\n",
      "062th iteration, cost= -0.8072565186127088\n",
      "063th iteration, cost= -0.8072587619818213\n",
      "064th iteration, cost= -0.8072592676387597\n",
      "065th iteration, cost= -0.8072599305411806\n",
      "066th iteration, cost= -0.8072605934436394\n",
      "067th iteration, cost= -0.807261256346136\n",
      "068th iteration, cost= -0.8072639641178336\n",
      "069th iteration, cost= -0.8072652899231685\n",
      "070th iteration, cost= -0.8072666157286549\n",
      "071th iteration, cost= -0.8072693235010979\n",
      "072th iteration, cost= -0.8072714096257606\n",
      "073th iteration, cost= -0.8072720725287481\n",
      "074th iteration, cost= -0.8072727354317734\n",
      "075th iteration, cost= -0.8072733983348365\n",
      "076th iteration, cost= -0.8072740612379374\n",
      "077th iteration, cost= -0.8072747241410766\n",
      "078th iteration, cost= -0.8072774319145415\n",
      "079th iteration, cost= -0.8072787577211601\n",
      "080th iteration, cost= -0.8072800835279308\n",
      "081th iteration, cost= -0.8072827913021393\n",
      "082th iteration, cost= -0.8072842145242769\n",
      "083th iteration, cost= -0.8072848774278684\n",
      "084th iteration, cost= -0.8072855403314966\n",
      "085th iteration, cost= -0.8072862032351635\n",
      "086th iteration, cost= -0.8072868661388682\n",
      "087th iteration, cost= -0.8072875290426109\n",
      "088th iteration, cost= -0.8072902368177259\n",
      "089th iteration, cost= -0.8072915626255515\n",
      "090th iteration, cost= -0.8072922255295211\n",
      "091th iteration, cost= -0.8072949333052657\n",
      "092th iteration, cost= -0.8072975849219756\n",
      "093th iteration, cost= -0.8072976823361043\n",
      "094th iteration, cost= -0.8072983452403361\n",
      "095th iteration, cost= -0.807299008144606\n",
      "096th iteration, cost= -0.8072996710489132\n",
      "097th iteration, cost= -0.8073003339532583\n",
      "098th iteration, cost= -0.8073009968576418\n",
      "099th iteration, cost= -0.8073037046345173\n",
      "100th iteration, cost= -0.8073043675390515\n",
      "101th iteration, cost= -0.8073048731912642\n",
      "102th iteration, cost= -0.8073055360957759\n",
      "103th iteration, cost= -0.8073060417477842\n",
      "104th iteration, cost= -0.8073087495245915\n",
      "105th iteration, cost= -0.8073104872534425\n",
      "106th iteration, cost= -0.8073111501582761\n",
      "107th iteration, cost= -0.8073118130631479\n",
      "108th iteration, cost= -0.8073123187145853\n",
      "109th iteration, cost= -0.8073128243658783\n",
      "110th iteration, cost= -0.8073134872706669\n",
      "111th iteration, cost= -0.807316195048376\n",
      "112th iteration, cost= -0.8073175208582921\n",
      "113th iteration, cost= -0.8073181837633066\n",
      "114th iteration, cost= -0.8073188466683586\n",
      "115th iteration, cost= -0.8073215544468088\n",
      "116th iteration, cost= -0.8073236405749424\n",
      "117th iteration, cost= -0.8073249663855323\n",
      "118th iteration, cost= -0.8073256292908839\n",
      "119th iteration, cost= -0.8073262921962734\n",
      "120th iteration, cost= -0.8073269551017007\n",
      "121th iteration, cost= -0.8073296628811636\n",
      "122th iteration, cost= -0.8073309886923571\n",
      "123th iteration, cost= -0.8073316515980102\n",
      "124th iteration, cost= -0.8073323145037012\n",
      "125th iteration, cost= -0.807335022283903\n",
      "126th iteration, cost= -0.8073364455072151\n",
      "127th iteration, cost= -0.8073371084130919\n",
      "128th iteration, cost= -0.807337771319006\n",
      "129th iteration, cost= -0.8073384342249581\n",
      "130th iteration, cost= -0.8073390971309482\n",
      "131th iteration, cost= -0.8073397600369756\n",
      "132th iteration, cost= -0.8073424678180758\n",
      "133th iteration, cost= -0.8073437936304698\n",
      "134th iteration, cost= -0.8073444565367235\n",
      "135th iteration, cost= -0.8073451194430146\n",
      "136th iteration, cost= -0.8073464452557098\n",
      "137th iteration, cost= -0.807349153037773\n",
      "138th iteration, cost= -0.8073505762615346\n",
      "139th iteration, cost= -0.8073512391680866\n",
      "140th iteration, cost= -0.8073519020746759\n",
      "141th iteration, cost= -0.8073525649813033\n",
      "142th iteration, cost= -0.8073532278879678\n",
      "143th iteration, cost= -0.8073559356708155\n",
      "144th iteration, cost= -0.8073572614844832\n",
      "145th iteration, cost= -0.807357924391374\n",
      "146th iteration, cost= -0.807358430037931\n",
      "147th iteration, cost= -0.8073595985911504\n",
      "148th iteration, cost= -0.8073623063741485\n",
      "149th iteration, cost= -0.807364044120254\n",
      "150th iteration, cost= -0.8073647070274422\n",
      "151th iteration, cost= -0.8073653699346686\n",
      "152th iteration, cost= -0.8073658755804478\n",
      "153th iteration, cost= -0.8073685833645208\n",
      "154th iteration, cost= -0.807369751917118\n",
      "155th iteration, cost= -0.8073704148244103\n",
      "156th iteration, cost= -0.8073710777317396\n",
      "157th iteration, cost= -0.8073737855161428\n",
      "158th iteration, cost= -0.807376437146287\n",
      "159th iteration, cost= -0.8073771974628474\n",
      "160th iteration, cost= -0.8073778603704749\n",
      "161th iteration, cost= -0.80737852327814\n",
      "162th iteration, cost= -0.8073791861858425\n",
      "163th iteration, cost= -0.8073818378170287\n",
      "164th iteration, cost= -0.8073825568791031\n",
      "165th iteration, cost= -0.807383219786993\n",
      "166th iteration, cost= -0.8073838826949207\n",
      "167th iteration, cost= -0.8073845456028856\n",
      "168th iteration, cost= -0.8073858714189286\n",
      "169th iteration, cost= -0.8073885792052944\n",
      "170th iteration, cost= -0.8073886766134849\n",
      "171th iteration, cost= -0.8073900024298213\n",
      "172th iteration, cost= -0.8073906653380459\n",
      "173th iteration, cost= -0.8073913282463083\n",
      "174th iteration, cost= -0.8073919911546084\n",
      "175th iteration, cost= -0.8073946989416407\n",
      "176th iteration, cost= -0.8073960247585773\n",
      "177th iteration, cost= -0.807396687667102\n",
      "178th iteration, cost= -0.807397350575664\n",
      "179th iteration, cost= -0.8073980134842633\n",
      "180th iteration, cost= -0.8074007212721404\n",
      "181th iteration, cost= -0.8074033729073625\n",
      "182th iteration, cost= -0.8074034703146102\n",
      "183th iteration, cost= -0.8074041332234689\n",
      "184th iteration, cost= -0.8074047961323649\n",
      "185th iteration, cost= -0.8074054590412992\n",
      "186th iteration, cost= -0.80740811067741\n",
      "187th iteration, cost= -0.8074094926482691\n",
      "188th iteration, cost= -0.8074101555574276\n",
      "189th iteration, cost= -0.8074108184666232\n",
      "190th iteration, cost= -0.8074114813758566\n",
      "191th iteration, cost= -0.8074119870167443\n",
      "192th iteration, cost= -0.8074146948060594\n",
      "193th iteration, cost= -0.8074169382096069\n",
      "194th iteration, cost= -0.807417601119099\n",
      "195th iteration, cost= -0.8074182640286285\n",
      "196th iteration, cost= -0.8074187696687976\n",
      "197th iteration, cost= -0.8074214774588884\n",
      "198th iteration, cost= -0.8074226460081767\n",
      "199th iteration, cost= -0.8074233089177708\n",
      "200th iteration, cost= -0.8074239718274029\n",
      "201th iteration, cost= -0.8074246347370719\n",
      "202th iteration, cost= -0.8074273425275988\n",
      "203th iteration, cost= -0.8074294286626926\n",
      "204th iteration, cost= -0.807430091572583\n",
      "205th iteration, cost= -0.8074307544825103\n",
      "206th iteration, cost= -0.8074314173924758\n",
      "207th iteration, cost= -0.807434125183776\n",
      "208th iteration, cost= -0.8074354510040417\n",
      "209th iteration, cost= -0.8074361139142306\n",
      "210th iteration, cost= -0.8074367768244571\n",
      "211th iteration, cost= -0.8074374397347203\n",
      "212th iteration, cost= -0.8074381026450219\n",
      "213th iteration, cost= -0.8074408104372731\n",
      "214th iteration, cost= -0.8074428965734006\n",
      "215th iteration, cost= -0.8074442223944813\n",
      "216th iteration, cost= -0.807444885305078\n",
      "217th iteration, cost= -0.8074475369478389\n",
      "218th iteration, cost= -0.8074482560088472\n",
      "219th iteration, cost= -0.8074489189196298\n",
      "220th iteration, cost= -0.8074495818304501\n",
      "221th iteration, cost= -0.8074502447413077\n",
      "222th iteration, cost= -0.8074509076522025\n",
      "223th iteration, cost= -0.8074536154460638\n",
      "224th iteration, cost= -0.8074562670904626\n",
      "225th iteration, cost= -0.8074563644942648\n",
      "226th iteration, cost= -0.8074570274054177\n",
      "227th iteration, cost= -0.8074576903166079\n",
      "228th iteration, cost= -0.807458353227835\n",
      "229th iteration, cost= -0.807461061022578\n",
      "230th iteration, cost= -0.8074617239339537\n",
      "231th iteration, cost= -0.8074623868453671\n",
      "232th iteration, cost= -0.807463049756818\n",
      "233th iteration, cost= -0.8074637126683056\n",
      "234th iteration, cost= -0.8074643755798313\n",
      "235th iteration, cost= -0.8074655441265991\n",
      "236th iteration, cost= -0.8074682519219953\n",
      "237th iteration, cost= -0.8074704953370518\n",
      "238th iteration, cost= -0.8074711582488716\n",
      "239th iteration, cost= -0.8074734953413271\n",
      "240th iteration, cost= -0.8074750345913787\n",
      "241th iteration, cost= -0.8074755402254254\n",
      "242th iteration, cost= -0.8074762031373087\n",
      "243th iteration, cost= -0.8074768660492296\n",
      "244th iteration, cost= -0.8074788547852153\n",
      "245th iteration, cost= -0.8074815625819203\n",
      "246th iteration, cost= -0.8074829858083583\n",
      "247th iteration, cost= -0.8074836487205733\n",
      "248th iteration, cost= -0.8074843116328254\n",
      "249th iteration, cost= -0.807487019430076\n",
      "250th iteration, cost= -0.8074883452549135\n",
      "251th iteration, cost= -0.8074890081673884\n",
      "252th iteration, cost= -0.8074896710799001\n",
      "253th iteration, cost= -0.8074903339924497\n",
      "254th iteration, cost= -0.8074909969050361\n",
      "255th iteration, cost= -0.8074937047032309\n",
      "256th iteration, cost= -0.8074963563543932\n",
      "257th iteration, cost= -0.8074964537555687\n",
      "258th iteration, cost= -0.8074971166684118\n",
      "259th iteration, cost= -0.8074977795812925\n",
      "260th iteration, cost= -0.8075004873802516\n",
      "261th iteration, cost= -0.8075018132063456\n",
      "262th iteration, cost= -0.8075024761194481\n",
      "263th iteration, cost= -0.8075031390325885\n",
      "264th iteration, cost= -0.8075038019457657\n",
      "265th iteration, cost= -0.8075044648589801\n",
      "266th iteration, cost= -0.807505127772232\n",
      "267th iteration, cost= -0.8075078355722446\n",
      "268th iteration, cost= -0.8075092587994334\n",
      "269th iteration, cost= -0.807509921712867\n",
      "270th iteration, cost= -0.8075105846263374\n",
      "271th iteration, cost= -0.8075132924268917\n",
      "272th iteration, cost= -0.8075146182541659\n",
      "273th iteration, cost= -0.8075152811678585\n",
      "274th iteration, cost= -0.8075159440815887\n",
      "275th iteration, cost= -0.8075166069953557\n",
      "276th iteration, cost= -0.8075172699091601\n",
      "277th iteration, cost= -0.8075179328230015\n",
      "278th iteration, cost= -0.8075204833399925\n",
      "279th iteration, cost= -0.8075227267661753\n",
      "280th iteration, cost= -0.8075240525943328\n",
      "281th iteration, cost= -0.8075263896800691\n",
      "282th iteration, cost= -0.8075279289393884\n",
      "283th iteration, cost= -0.8075284345677981\n",
      "284th iteration, cost= -0.8075290974819579\n",
      "285th iteration, cost= -0.8075297603961539\n",
      "286th iteration, cost= -0.8075304233103875\n",
      "287th iteration, cost= -0.8075310862246582\n",
      "288th iteration, cost= -0.8075337940271544\n",
      "289th iteration, cost= -0.8075358801691079\n",
      "290th iteration, cost= -0.8075372059981227\n",
      "291th iteration, cost= -0.8075398576565983\n",
      "292th iteration, cost= -0.8075412396306525\n",
      "293th iteration, cost= -0.8075419025454001\n",
      "294th iteration, cost= -0.8075425654601848\n",
      "295th iteration, cost= -0.8075432283750066\n",
      "296th iteration, cost= -0.8075438912898654\n",
      "297th iteration, cost= -0.8075452171196948\n",
      "298th iteration, cost= -0.8075479249239979\n",
      "299th iteration, cost= -0.8075486852368423\n",
      "300th iteration, cost= -0.8075500110670332\n",
      "301th iteration, cost= -0.8075526627278599\n",
      "302th iteration, cost= -0.8075540447023235\n",
      "303th iteration, cost= -0.8075547076176588\n",
      "304th iteration, cost= -0.807555370533031\n",
      "305th iteration, cost= -0.8075560334484403\n",
      "306th iteration, cost= -0.8075566963638869\n",
      "307th iteration, cost= -0.80755735927937\n",
      "308th iteration, cost= -0.807558685110448\n",
      "309th iteration, cost= -0.8075613929164448\n",
      "310th iteration, cost= -0.8075621532290492\n",
      "311th iteration, cost= -0.8075634790604885\n",
      "312th iteration, cost= -0.8075661307238111\n",
      "313th iteration, cost= -0.8075675126987186\n",
      "314th iteration, cost= -0.8075681756146774\n",
      "315th iteration, cost= -0.8075688385306737\n",
      "316th iteration, cost= -0.8075695014467068\n",
      "317th iteration, cost= -0.8075701643627768\n",
      "318th iteration, cost= -0.8075708272788839\n",
      "319th iteration, cost= -0.8075718385265855\n",
      "320th iteration, cost= -0.8075745463334625\n",
      "321th iteration, cost= -0.8075769470639448\n",
      "322th iteration, cost= -0.8075792841429101\n",
      "323th iteration, cost= -0.8075801604950221\n",
      "324th iteration, cost= -0.8075813290342289\n",
      "325th iteration, cost= -0.8075819919506508\n",
      "326th iteration, cost= -0.8075826548671101\n",
      "327th iteration, cost= -0.807583317783606\n",
      "328th iteration, cost= -0.8075846436167089\n",
      "329th iteration, cost= -0.8075873514251624\n",
      "330th iteration, cost= -0.8075894375705148\n",
      "331th iteration, cost= -0.8075920892378845\n",
      "332th iteration, cost= -0.8075934712132221\n",
      "333th iteration, cost= -0.8075947970472\n",
      "334th iteration, cost= -0.8075954599642441\n",
      "335th iteration, cost= -0.8075961228813252\n",
      "336th iteration, cost= -0.8075967857984436\n",
      "337th iteration, cost= -0.8075981116327905\n",
      "338th iteration, cost= -0.8076008194429265\n",
      "339th iteration, cost= -0.8076029055892797\n",
      "340th iteration, cost= -0.8076055572591351\n",
      "341th iteration, cost= -0.8076082650701316\n",
      "342th iteration, cost= -0.8076082650701316\n"
     ]
    }
   ],
   "source": [
    "params_t = torch.tensor([np.pi/2, np.pi], requires_grad=True)\n",
    "# params_t = torch.tensor(params, requires_grad=True, device=torch.device('mps'))\n",
    "\n",
    "Phi_global = torch.tensor([np.pi*2])\n",
    "\n",
    "\n",
    "opt = torch.optim.LBFGS([params_t])\n",
    "# opt = torch.optim.Adam([params_t])\n",
    "\n",
    "steps = 1000\n",
    "\n",
    "f_logs = [Cost_function(params_t).item()]\n",
    "ftol = 1e-12\n",
    "\n",
    "def closure():\n",
    "    opt.zero_grad()\n",
    "    loss = Cost_function(params_t)\n",
    "    loss.backward()\n",
    "    # print(loss)\n",
    "    return loss\n",
    "\n",
    "for i in range(steps):\n",
    "    opt.step(closure)\n",
    "    fval = Cost_function(opt.param_groups[0]['params'][0]).item()\n",
    "    print(f\"{i+1:03d}th iteration, cost=\", fval)\n",
    "    f_logs.append(fval)\n",
    "    if np.abs((fval-f_logs[-2])/fval) < ftol:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Index\n",
    "class DataIndex(Enum):\n",
    "    BEFORE = 0\n",
    "    PHI = 0\n",
    "    CFI = 1\n",
    "    PARAS = 2\n",
    "    THETA_X = 2\n",
    "    PHI_Z = 3\n",
    "    \n",
    "    \n",
    "# == BFGS -> Return Data_set:[phi, CFI, 6-Paras] ==\n",
    "def run_optimization(sweep_data, initial_parameters, gamma_ps, iterations, circuit_select):\n",
    "    \"\"\" \n",
    "    Main function to perform optimization over a range of phi values using the BFGS algorithm.\n",
    "    \n",
    "    Args:\n",
    "        sweep_data (tuple): (start, end, step) values for the phi sweep.\n",
    "        initial_parameters (numpy_array): Initial parameters for optimization.\n",
    "        gamma_ps (int): Gamma value for post-selection.\n",
    "        iterations (int): Number of iterations for the optimization.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: A 3-D array containing phi, CFI, and optimized parameters after each iteration.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create Data array\n",
    "    PHI = np.arange(sweep_data[0], sweep_data[1], sweep_data[2])\n",
    "    Data = np.zeros((iterations + 1, len(PHI), len(initial_parameters) + 2)) \n",
    "    Data[:, :, DataIndex.PHI.value] = PHI.squeeze() # Append PHI in to 0th col\n",
    "    \n",
    "    # Set global variables\n",
    "    global Gamma_ps, Phi_global, Paras_global, Tau_global\n",
    "    Gamma_ps = gamma_ps \n",
    "    \n",
    "    # Declare Paras temp \n",
    "    Paras_Temporary = 0\n",
    "    \n",
    "    # Store initial CFI data and parameters\n",
    "    for idx in range(len(PHI)):\n",
    "        Data[DataIndex.BEFORE.value][idx][DataIndex.CFI.value] = -Cost_function(initial_parameters, circuit_select)\n",
    "        Data[DataIndex.BEFORE.value][idx][DataIndex.PARAS.value:] = initial_parameters\n",
    "        \n",
    "    # Optimize begin\n",
    "    for iteration in range(1, iterations + 1):\n",
    "        for phi_idx, phi_current in enumerate(PHI):\n",
    "            # Determine initial parameters based on the iteration\n",
    "            if iteration == 1:\n",
    "                Paras_Temporary = initial_parameters\n",
    "\n",
    "            else:\n",
    "                Paras_Temporary = Data[iteration][phi_idx][DataIndex.PARAS.value:]\n",
    "            \n",
    "            # Update the global Phi value\n",
    "            Phi_global = phi_current\n",
    "\n",
    "            # Determine constraints\n",
    "            Constraints = get_constraints(phi_current, gamma_ps, Tau_global)\n",
    "\n",
    "            # Optimize the data\n",
    "            N = int(phi_current / pnp.pi) * pnp.pi\n",
    "            if Gamma_ps == 8e-1:\n",
    "                if Tau_global == 0:\n",
    "                    Paras_Temporary = pnp.array([pnp.pi/2, pnp.pi/2])\n",
    "\n",
    "                elif (pnp.pi/2 + N <= phi_current <= 2.1 + N):   # Up next\n",
    "                    Paras_Temporary = pnp.array([pnp.pi/2, 1])\n",
    "\n",
    "            Paras_global = Paras_Temporary\n",
    "\n",
    "            Result_BFGS = BFGS(Paras_Temporary, Constraints, circuit_select)\n",
    "            Data[iteration][phi_idx][DataIndex.CFI.value] = -Result_BFGS.fun\n",
    "            Data[iteration][phi_idx][DataIndex.PARAS.value:] = Result_BFGS.x\n",
    "            \n",
    "    return Data\n",
    "\n",
    "def BFGS(initial_parameters, constraints, circuit_select):\n",
    "    \"\"\"\n",
    "    Perform the BFGS optimization algorithm.\n",
    "\n",
    "    Args:\n",
    "        initial_parameters (numpy_array): The starting point for the optimization.\n",
    "        constraints (list of tuple): Bounds on the variables for the optimization.\n",
    "\n",
    "    Returns:\n",
    "        OptimizeResult: The result of the optimization process.\n",
    "    \"\"\"\n",
    "    \n",
    "    optimization_result = torch.optim.LBFGS(\n",
    "                fun = Cost_function, \n",
    "                x0 = initial_parameters, \n",
    "                args = (circuit_select,),\n",
    "                method = 'L-BFGS-B', \n",
    "                bounds = constraints,\n",
    "                jac = gradient,\n",
    "                hess = hessian,\n",
    "                tol = 1e-20,\n",
    "                options={\n",
    "                    'ftol': 1e-20, \n",
    "                    'gtol': 1e-20\n",
    "                }\n",
    "            )\n",
    "    \n",
    "    \n",
    "    return optimization_result\n",
    "\n",
    "def get_constraints(phi_current, gamma_ps, tau_current):\n",
    "    \"\"\"\n",
    "    Calculate the constraints for the optimization based on current phi, gamma and tau values.\n",
    "\n",
    "    Args:\n",
    "        phi_current (float): The current value of phi in the optimization loop.\n",
    "        gamma_ps (float): Gamma value for post-selection.\n",
    "        tau_current (float): Current value of tau.\n",
    "\n",
    "    Returns:\n",
    "        list of tuple: Constraints for the optimization variables.\n",
    "    \"\"\"\n",
    "    \n",
    "    N = 2*np.pi * int(phi_current / (2*np.pi))\n",
    "    if Gamma_ps == 8e-1:\n",
    "        if tau_current == 0:\n",
    "            return [\n",
    "                (np.pi/2, np.pi/2),\n",
    "                (-np.pi/2, 3*np.pi/2)\n",
    "            ]\n",
    "\n",
    "        elif tau_current == (5e-2):\n",
    "            if phi_current < 0.45 + N:\n",
    "                return [(np.pi/2, np.pi)] * 2\n",
    "            elif phi_current <= 1.02 + N:\n",
    "                return [(np.pi/2, 3.70)] * 2\n",
    "            elif phi_current <= 1.57 + N:\n",
    "                return [(np.pi/2, 4.24006637)] * 2\n",
    "            elif 3.00 + N <= phi_current <= 3.67 + N:\n",
    "                return [(np.pi/2, np.pi/2), (0.32993364, 0.99993333)]\n",
    "            elif 3.69 + N <= phi_current <= 4.0 + N:\n",
    "                return [(np.pi/2, np.pi/2), (1.01993369, 1.32993365)]\n",
    "            elif 4.03 + N <= phi_current <= 4.22 + N:\n",
    "                return [(np.pi/2, np.pi/2), (1.35993364, 1.54993374)]\n",
    "            elif 4.24 + N <= phi_current <= 4.69 + N:\n",
    "                return [(np.pi/2, np.pi/2), (1.56993364, 2.01993363)]\n",
    "            elif (4.82) + N <= phi_current <= (5.5) + N:\n",
    "                return [(np.pi/2, np.pi/2), (1.20688106, 1.88688109)]\n",
    "            elif 5.5 + N <= phi_current <= (6.0) + N:\n",
    "                return [(np.pi/2, np.pi/2), (1.88688109, 2.38688106)]\n",
    "            elif 6.05 + N <= phi_current <= (6.15) + N:\n",
    "                return [(np.pi/2, np.pi/2), (2.43688084, 2.53688103)]\n",
    "\n",
    "        elif tau_current == 2e-1:\n",
    "            if phi_current <= 0.5 + N:\n",
    "                return [(0, np.pi)] * 2\n",
    "            elif phi_current < 1.58 + N:\n",
    "                return [(-0.8439553621272445, 3.9939553536152146)] * 2 \n",
    "            elif phi_current < 2.42 + N:\n",
    "                return [(-0.8439553621272445, 3.9939553536152146)] * 2 \n",
    "            elif phi_current < 4 + N:\n",
    "                return [(0, np.pi/2)] * 2 \n",
    "            elif phi_current < (1.57 + 3.14) + N:\n",
    "                return [(np.pi/2, np.pi)] * 2 \n",
    "        \n",
    "        elif tau_current == 5e-1:\n",
    "            return [(-5e-1, pnp.pi + 6e-1 )] * 2\n",
    "        \n",
    "        elif 1 <= tau_current <= 4:\n",
    "            return [(-5e-1, pnp.pi + 35e-2)] * 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
